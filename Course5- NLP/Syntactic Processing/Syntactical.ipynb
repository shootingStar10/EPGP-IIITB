{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Step 1: Setting Up\n",
        "First, ensure you have NLTK installed and download the necessary resources."
      ],
      "metadata": {
        "id": "6sl0AxLSkiNA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VQkf8g-bkVHp",
        "outputId": "bc3398a7-a056-4ede-e9c1-529b1397a1ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.5)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download('punkt')  # For tokenization\n",
        "nltk.download('averaged_perceptron_tagger')  # For POS tagging pos_tag\n",
        "nltk.download('maxent_ne_chunker')  # For chunking\n",
        "nltk.download('words')  # For chunking\n",
        "nltk.download('wordnet') # for lemmatization\n",
        "nltk.download('stopwords')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: Initializing Sample Data\n",
        "Let's start with an array of sentences."
      ],
      "metadata": {
        "id": "iyJwexrSkvgJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample array of sentences\n",
        "sentences = [\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"Artificial Intelligence is transforming the world.\",\n",
        "    \"Natural Language Processing is a part of AI.\"\n",
        "]\n"
      ],
      "metadata": {
        "id": "TPU65_COkz7c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = \"abc\"  # characters to be replaced\n",
        "y = \"xyz\"  # characters to replace the ones in x\n",
        "\n",
        "# Create the translation table\n",
        "translation_table = str.maketrans(x, y)\n",
        "\n",
        "# Now, use translate() to apply the translation to a string\n",
        "text = \"I have a cat, a bat, and a cap.\"\n",
        "translated_text = text.translate(translation_table)\n",
        "\n",
        "print(translated_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ttFflWtqjyo2",
        "outputId": "c5b3b14a-f5d6-47f2-af86-6fa4d9995992"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I hxve x zxt, x yxt, xnd x zxp.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "str.maketrans()\n",
        "Purpose: maketrans() is a method used to create a translation table that maps characters from one set to another. It can also map characters to None, which means removing those characters.\n",
        "Syntax: str.maketrans(x, y, z)\n",
        "x: A string of characters to be replaced.\n",
        "y: A string of characters to replace the ones in x (must have the same length as x).\n",
        "z: A string of characters to be deleted (i.e., mapped to None).\n",
        "In your code, str.maketrans('', '', string.punctuation):\n",
        "\n",
        "It doesn’t replace any characters because the first two arguments are empty strings.\n",
        "The third argument, string.punctuation, tells Python to map all punctuation characters to None, which effectively means \"remove these characters.\""
      ],
      "metadata": {
        "id": "x7yXy2xakHJN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "translate()\n",
        "Purpose: translate() is a string method that takes a translation table (created by maketrans()) and applies it to a string.\n",
        "Syntax: string.translate(translation_table)\n",
        "translation_table: The table created by maketrans() that tells Python which characters to replace or delete."
      ],
      "metadata": {
        "id": "yMTwNTlpkKc3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3: Tokenization, Normalization, and Sentence Segmentation\n",
        "We'll tokenize each sentence, normalize the text, and perform sentence segmentation."
      ],
      "metadata": {
        "id": "05_MAGEvk2Ft"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
        "# Function to normalize text\n",
        "def normalize_text(text):\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove punctuation                (character you want to replace) (corresponding characters for substitution)  remove punc characters from text (delete chars from text - 3rd input)\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation)) # replace the with empty character and remove punctuations\n",
        "    return text\n",
        "\n",
        "# Tokenization, normalization, and sentence segmentation\n",
        "def preprocess_sentences(sentences):\n",
        "    all_tokens = []\n",
        "    for sentence in sentences:\n",
        "        # tokenization\n",
        "        # Normalize the sentence\n",
        "        normalized_sentence = normalize_text(sentence)\n",
        "        # Tokenize the normalized sentence\n",
        "        tokens = word_tokenize(normalized_sentence)\n",
        "        all_tokens.append(tokens)\n",
        "    return all_tokens\n",
        "\n",
        "# Preprocess the sample sentences\n",
        "tokens_list = preprocess_sentences(sentences)\n",
        "for tokens in tokens_list:\n",
        "    print(\"Tokens:\", tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9B3TGH5Nk5wD",
        "outputId": "d9b38c61-f698-4ac0-8246-a55e42d3b684"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens: ['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog']\n",
            "Tokens: ['artificial', 'intelligence', 'is', 'transforming', 'the', 'world']\n",
            "Tokens: ['natural', 'language', 'processing', 'is', 'a', 'part', 'of', 'ai']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4: POS Tagging\n",
        "We will perform POS tagging on the tokenized text."
      ],
      "metadata": {
        "id": "8H7fNYkAk9SV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to perform POS tagging on tokenized text\n",
        "def pos_tagging(tokens_list):\n",
        "    pos_tags_list = []\n",
        "    for tokens in tokens_list:\n",
        "        pos_tags = nltk.pos_tag(tokens)\n",
        "        pos_tags_list.append(pos_tags)\n",
        "    return pos_tags_list\n",
        "\n",
        "# Perform POS tagging\n",
        "pos_tags_list = pos_tagging(tokens_list)\n",
        "for pos_tags in pos_tags_list:\n",
        "    print(\"POS Tags:\", pos_tags)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NfvnjLFblAd8",
        "outputId": "0efd6d84-f056-47d0-cd86-f6e97350bc9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "POS Tags: [('the', 'DT'), ('quick', 'JJ'), ('brown', 'NN'), ('fox', 'NN'), ('jumps', 'VBZ'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN')]\n",
            "POS Tags: [('artificial', 'JJ'), ('intelligence', 'NN'), ('is', 'VBZ'), ('transforming', 'VBG'), ('the', 'DT'), ('world', 'NN')]\n",
            "POS Tags: [('natural', 'JJ'), ('language', 'NN'), ('processing', 'NN'), ('is', 'VBZ'), ('a', 'DT'), ('part', 'NN'), ('of', 'IN'), ('ai', 'NN')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 5 Chunking, or shallow parsing, involves grouping words into chunks based on POS tags.\n",
        "\n"
      ],
      "metadata": {
        "id": "BkChQbmtlDA5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to perform chunking\n",
        "def chunking(pos_tags_list):\n",
        "    chunks_list = []\n",
        "    # Define a simple chunk grammar\n",
        "    # \"NP\" stands for Noun Phrase\n",
        "    # {<DT>?<JJ>*<NN>} is the pattern:\n",
        "    # - <DT>? means an optional determiner (e.g., \"the\", \"a\")\n",
        "    #   - The `?` symbol means the determiner is optional and may appear zero or one time.\n",
        "    # - <JJ>* means zero or more adjectives (e.g., \"quick\", \"brown\")\n",
        "    #   - The `*` symbol means there can be zero or more adjectives.\n",
        "    # - <NN> means a singular noun (e.g., \"fox\", \"dog\")\n",
        "    chunk_grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
        "    chunk_parser = nltk.RegexpParser(chunk_grammar)\n",
        "\n",
        "    for pos_tags in pos_tags_list:\n",
        "        # Parse the POS tagged sentence according to the chunk grammar\n",
        "        tree = chunk_parser.parse(pos_tags)\n",
        "        chunks_list.append(tree)\n",
        "    return chunks_list\n",
        "\n",
        "# Perform chunking\n",
        "chunks_list = chunking(pos_tags_list)\n",
        "print(chunks_list)\n",
        "for tree in chunks_list:\n",
        "    print(\"Chunks:\")\n",
        "    tree.pretty_print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F-tJSHU7lndM",
        "outputId": "cbf1cad3-64d0-421e-df76-e3e0f2b947ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Tree('S', [Tree('NP', [('the', 'DT'), ('quick', 'JJ'), ('brown', 'NN')]), Tree('NP', [('fox', 'NN')]), ('jumps', 'VBZ'), ('over', 'IN'), Tree('NP', [('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN')])]), Tree('S', [Tree('NP', [('artificial', 'JJ'), ('intelligence', 'NN')]), ('is', 'VBZ'), ('transforming', 'VBG'), Tree('NP', [('the', 'DT'), ('world', 'NN')])]), Tree('S', [Tree('NP', [('natural', 'JJ'), ('language', 'NN')]), Tree('NP', [('processing', 'NN')]), ('is', 'VBZ'), Tree('NP', [('a', 'DT'), ('part', 'NN')]), ('of', 'IN'), Tree('NP', [('ai', 'NN')])])]\n",
            "Chunks:\n",
            "                                     S                                 \n",
            "     ________________________________|______________________            \n",
            "    |        |              NP               NP             NP         \n",
            "    |        |       _______|________        |       _______|______     \n",
            "jumps/VBZ over/IN the/DT quick/JJ brown/NN fox/NN the/DT lazy/JJ dog/NN\n",
            "\n",
            "Chunks:\n",
            "                              S                                              \n",
            "   ___________________________|___________________________________            \n",
            "  |           |                        NP                         NP         \n",
            "  |           |                ________|_________            _____|_____      \n",
            "is/VBZ transforming/VBG artificial/JJ     intelligence/NN the/DT     world/NN\n",
            "\n",
            "Chunks:\n",
            "                                 S                                          \n",
            "   ______________________________|_______________________________________    \n",
            "  |      |               NP                   NP            NP           NP \n",
            "  |      |        _______|_______             |         ____|_____       |   \n",
            "is/VBZ of/IN natural/JJ     language/NN processing/NN a/DT     part/NN ai/NN\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to perform lemmatization on tokens\n",
        "def lemmatization(tokens_list):\n",
        "    lemmatizer = WordNetLemmatizer() #nltk.download('wordnet')\n",
        "    lemmatized_tokens_list = []\n",
        "    for tokens in tokens_list:\n",
        "        lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "        lemmatized_tokens_list.append(lemmatized_tokens)\n",
        "    return lemmatized_tokens_list\n",
        "\n",
        "# Perform lemmatization\n",
        "lemmatized_tokens_list = lemmatization(tokens_list)\n",
        "for lemmatized_tokens in lemmatized_tokens_list:\n",
        "    print(\"Lemmatized Tokens:\", lemmatized_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EWN8oTU5apgh",
        "outputId": "7ae0606f-1ac2-4535-c8f6-3978d50a083e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lemmatized Tokens: ['the', 'quick', 'brown', 'fox', 'jump', 'over', 'the', 'lazy', 'dog']\n",
            "Lemmatized Tokens: ['artificial', 'intelligence', 'is', 'transforming', 'the', 'world']\n",
            "Lemmatized Tokens: ['natural', 'language', 'processing', 'is', 'a', 'part', 'of', 'ai']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to perform lemmatization on tokens\n",
        "def lemmatization(tokens_list):\n",
        "    lemmatizer = WordNetLemmatizer() #nltk.download('wordnet')\n",
        "    lemmatized_tokens_list = []\n",
        "    for tokens in tokens_list:\n",
        "        lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "        lemmatized_tokens_list.append(lemmatized_tokens)\n",
        "    return lemmatized_tokens_list\n",
        "\n",
        "# Perform lemmatization\n",
        "lemmatized_tokens_list = lemmatization(tokens_list)\n",
        "for lemmatized_tokens in lemmatized_tokens_list:\n",
        "    print(\"Lemmatized Tokens:\", lemmatized_tokens)\n",
        "\n",
        "# Function to perform stemming on tokens\n",
        "def stemming(tokens_list):\n",
        "    stemmer = PorterStemmer()\n",
        "    stemmed_tokens_list = []\n",
        "    for tokens in tokens_list:\n",
        "        stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
        "        stemmed_tokens_list.append(stemmed_tokens)\n",
        "    return stemmed_tokens_list\n",
        "\n",
        "# Perform stemming\n",
        "stemmed_tokens_list = stemming(tokens_list)\n",
        "for stemmed_tokens in stemmed_tokens_list:\n",
        "    print(\"Stemmed Tokens:\", stemmed_tokens)\n",
        "\n",
        "# Function to remove stop words from tokens\n",
        "def remove_stop_words(tokens_list):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_tokens_list = []\n",
        "    for tokens in tokens_list:\n",
        "        filtered_tokens = [token for token in tokens if token.lower() not in stop_words]\n",
        "        filtered_tokens_list.append(filtered_tokens)\n",
        "    return filtered_tokens_list\n",
        "\n",
        "# Remove stop words\n",
        "filtered_tokens_list = remove_stop_words(tokens_list)\n",
        "for filtered_tokens in filtered_tokens_list:\n",
        "    print(\"Filtered Tokens (After Stop Word Removal):\", filtered_tokens)\n",
        "\n",
        "    #noun lemmatization if you are not giving postag parameter, then if there is ambigutiy it will prefer its noun"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p3a1UPnDoH0N",
        "outputId": "056bcd98-14c9-4dbd-9073-0955190025ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lemmatized Tokens: ['the', 'quick', 'brown', 'fox', 'jump', 'over', 'the', 'lazy', 'dog']\n",
            "Lemmatized Tokens: ['artificial', 'intelligence', 'is', 'transforming', 'the', 'world']\n",
            "Lemmatized Tokens: ['natural', 'language', 'processing', 'is', 'a', 'part', 'of', 'ai']\n",
            "Stemmed Tokens: ['the', 'quick', 'brown', 'fox', 'jump', 'over', 'the', 'lazi', 'dog']\n",
            "Stemmed Tokens: ['artifici', 'intellig', 'is', 'transform', 'the', 'world']\n",
            "Stemmed Tokens: ['natur', 'languag', 'process', 'is', 'a', 'part', 'of', 'ai']\n",
            "Filtered Tokens (After Stop Word Removal): ['quick', 'brown', 'fox', 'jumps', 'lazy', 'dog']\n",
            "Filtered Tokens (After Stop Word Removal): ['artificial', 'intelligence', 'transforming', 'world']\n",
            "Filtered Tokens (After Stop Word Removal): ['natural', 'language', 'processing', 'part', 'ai']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stemming with Porter and Snowball Stemmers\n",
        "Stemming algorithms aim to remove affixes from words to get to their base forms. NLTK provides implementations of two popular stemming algorithms: Porter and Snowball (also known as Porter2).\n",
        "\n",
        "1. Porter Stemmer\n",
        "The Porter stemmer is one of the most widely used stemming algorithms. It follows a set of heuristic rules to remove suffixes from words."
      ],
      "metadata": {
        "id": "73K-E4X1pUDE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# Initialize the Porter stemmer\n",
        "porter_stemmer = PorterStemmer()\n",
        "\n",
        "# Example words to stem\n",
        "words = [\"running\", \"ran\", \"cats\", \"trouble\", \"troubling\", \"friendship\"]\n",
        "\n",
        "# Stemming using Porter stemmer\n",
        "stemmed_words_porter = [porter_stemmer.stem(word) for word in words]\n",
        "\n",
        "print(\"Original words:\", words)\n",
        "print(\"Stemmed words (Porter):\", stemmed_words_porter)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "urMjNrRQpVHi",
        "outputId": "203e1f65-154f-4bc3-d42e-227dab074f78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original words: ['running', 'ran', 'cats', 'trouble', 'troubling', 'friendship']\n",
            "Stemmed words (Porter): ['run', 'ran', 'cat', 'troubl', 'troubl', 'friendship']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Snowball Stemmer (Porter2 Stemmer)\n",
        "The Snowball stemmer (or Porter2 stemmer) is an improved version of the Porter stemmer and supports stemming in multiple languages."
      ],
      "metadata": {
        "id": "o4IiyqwnpazV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "# Initialize the Snowball stemmer for English\n",
        "snowball_stemmer = SnowballStemmer(\"english\")\n",
        "\n",
        "# Example words to stem\n",
        "words = [\"running\", \"ran\", \"cats\", \"trouble\", \"troubling\", \"friendship\"]\n",
        "\n",
        "# Stemming using Snowball stemmer\n",
        "stemmed_words_snowball = [snowball_stemmer.stem(word) for word in words]\n",
        "\n",
        "print(\"Original words:\", words)\n",
        "print(\"Stemmed words (Snowball):\", stemmed_words_snowball)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yEgk828apeS_",
        "outputId": "f84d2c76-bd69-47fe-e781-f1ad37042160"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original words: ['running', 'ran', 'cats', 'trouble', 'troubling', 'friendship']\n",
            "Stemmed words (Snowball): ['run', 'ran', 'cat', 'troubl', 'troubl', 'friendship']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The main difference between lemmatization with and without POS tagging lies in how accurately the lemma (base form) of a word is determined based on its part of speech (POS).\n",
        "\n",
        "Lemmatization Without POS Tagging\n",
        "When lemmatizing without POS tagging:\n",
        "\n",
        "The lemmatizer assumes that every word is a noun by default.\n",
        "For example, without POS tagging:\n",
        "\"running\" → \"running\"\n",
        "\"cats\" → \"cat\"\n",
        "\"better\" → \"better\"\n",
        "Lemmatization With POS Tagging\n",
        "When lemmatizing with POS tagging:\n",
        "\n",
        "Each word is first assigned a specific POS tag (part of speech) using a POS tagger.\n",
        "Based on the POS tag, the lemmatizer can accurately determine the lemma of the word.\n",
        "For example, with POS tagging:\n",
        "\"running\" (verb) → \"run\"\n",
        "\"cats\" (noun) → \"cat\"\n",
        "\"better\" (adjective) → \"good\"\n",
        "Importance of POS Tagging in Lemmatization\n",
        "POS tagging is important in lemmatization because:\n",
        "\n",
        "Words can have multiple meanings and can function as different parts of speech (e.g., \"run\" can be a noun or a verb).\n",
        "The lemma of a word differs depending on its POS. For instance, the lemma of \"better\" is \"good\" when it is used as an adjective, but remains \"better\" when used as an adverb or verb.\n",
        "By tagging words with their POS before lemmatization, the lemmatizer can select the appropriate lemma form from a dictionary (such as WordNet) based on the word's part of speech, resulting in more accurate and meaningful base forms.\n",
        "Example\n",
        "Consider the word \"better\":\n",
        "\n",
        "Without POS tagging, lemmatization would default to \"better\" → \"better\" (assuming it's a noun).\n",
        "With POS tagging:\n",
        "If tagged as an adjective, \"better\" → \"good\".\n",
        "If tagged as an adverb or verb, \"better\" → \"better\".\n",
        "Here's a Python example demonstrating the difference:"
      ],
      "metadata": {
        "id": "TtZGB8t_pij4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "With pos tagging parameter - lemmatization can give better results than without pos tagging\n",
        "lemmatization (word, pos) ->correct root form"
      ],
      "metadata": {
        "id": "UtT4cFhibW-S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import pos_tag\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Initialize the WordNet lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Example sentence\n",
        "sentence = \"He is running faster than before and likes cats.\"\n",
        "\n",
        "# Tokenize the sentence\n",
        "tokens = word_tokenize(sentence)\n",
        "\n",
        "# POS tagging using NLTK\n",
        "pos_tags = pos_tag(tokens)\n",
        "\n",
        "print(\"Tokens:\", tokens)\n",
        "print(\"POS Tags:\", pos_tags)\n",
        "print()\n",
        "\n",
        "# Lemmatization without POS tagging\n",
        "lemmatized_words_without_pos = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "print(\"Lemmatized words without POS tagging:\", lemmatized_words_without_pos)\n",
        "print()\n",
        "\n",
        "# Function to convert NLTK POS tags to WordNet POS tags\n",
        "def nltk_to_wordnet_pos(nltk_tag):\n",
        "    if nltk_tag.startswith('J'):\n",
        "        return 'a'  # Adjective\n",
        "    elif nltk_tag.startswith('VBZ'):\n",
        "        return 'v'  # Verb\n",
        "    elif nltk_tag.startswith('N'):\n",
        "        return 'n'  # Noun\n",
        "    elif nltk_tag.startswith('R'):\n",
        "        return 'n'  # Adverb\n",
        "    else:\n",
        "        return None  # Default to noun\n",
        "\n",
        "# Lemmatization with POS tagging\n",
        "lemmatized_words_with_pos = []\n",
        "for word, tag in pos_tags:\n",
        "    wordnet_pos = nltk_to_wordnet_pos(tag) # pos tags that comes from nltk passes on wordnet -> wordnet tagging and it will go towards lemma form\n",
        "    if wordnet_pos:\n",
        "        lemma = lemmatizer.lemmatize(word, pos=wordnet_pos)\n",
        "    else:\n",
        "        lemma = lemmatizer.lemmatize(word)  # default to noun\n",
        "    lemmatized_words_with_pos.append(lemma)\n",
        "\n",
        "print(\"Lemmatized words with POS tagging:\", lemmatized_words_with_pos)\n",
        "# is am are  - pos tag - verb form v\n",
        "# noun\n",
        "# destination tag\n",
        "\n",
        "# Verb form  be"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XMQA2uJvqAui",
        "outputId": "ddf923a3-1009-465c-bf0f-b85fe9c67d77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens: ['He', 'is', 'running', 'faster', 'than', 'before', 'and', 'likes', 'cats', '.']\n",
            "POS Tags: [('He', 'PRP'), ('is', 'VBZ'), ('running', 'VBG'), ('faster', 'RBR'), ('than', 'IN'), ('before', 'RB'), ('and', 'CC'), ('likes', 'JJ'), ('cats', 'NNS'), ('.', '.')]\n",
            "\n",
            "Lemmatized words without POS tagging: ['He', 'is', 'running', 'faster', 'than', 'before', 'and', 'like', 'cat', '.']\n",
            "\n",
            "Lemmatized words with POS tagging: ['He', 'be', 'run', 'faster', 'than', 'before', 'and', 'likes', 'cat', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using Pretrained Taggers\n"
      ],
      "metadata": {
        "id": "uacAdzpTrixK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Sample sentences\n",
        "sentences = [\n",
        "    \"He walked quickly to the store.\",\n",
        "    \"She sings beautifully every morning.\",\n",
        "    \"The old man greeted us warmly.\",\n",
        "    \"Their ideas seemed quite innovative.\",\n",
        "    \"I saw a man with a telescope.\"\n",
        "]\n",
        "\n",
        "# Tokenize and tag each sentence\n",
        "tagged_sentences = []\n",
        "for sentence in sentences:\n",
        "    tokens = word_tokenize(sentence)\n",
        "    tags = nltk.pos_tag(tokens)\n",
        "    tagged_sentences.append(tags)\n",
        "\n",
        "# Print POS tagged sentences\n",
        "for i, tags in enumerate(tagged_sentences, start=1):\n",
        "    print(f\"Sentence {i}:\")\n",
        "    print(tags)\n",
        "    print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fLPzpuR0rjtY",
        "outputId": "24c9cbeb-7036-409a-9f9a-567f08f1f4b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence 1:\n",
            "[('He', 'PRP'), ('walked', 'VBD'), ('quickly', 'RB'), ('to', 'TO'), ('the', 'DT'), ('store', 'NN'), ('.', '.')]\n",
            "\n",
            "Sentence 2:\n",
            "[('She', 'PRP'), ('sings', 'VBZ'), ('beautifully', 'RB'), ('every', 'DT'), ('morning', 'NN'), ('.', '.')]\n",
            "\n",
            "Sentence 3:\n",
            "[('The', 'DT'), ('old', 'JJ'), ('man', 'NN'), ('greeted', 'VBD'), ('us', 'PRP'), ('warmly', 'RB'), ('.', '.')]\n",
            "\n",
            "Sentence 4:\n",
            "[('Their', 'PRP$'), ('ideas', 'NNS'), ('seemed', 'VBD'), ('quite', 'RB'), ('innovative', 'JJ'), ('.', '.')]\n",
            "\n",
            "Sentence 5:\n",
            "[('I', 'PRP'), ('saw', 'VBD'), ('a', 'DT'), ('man', 'NN'), ('with', 'IN'), ('a', 'DT'), ('telescope', 'NN'), ('.', '.')]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Additional Rules for Custom Tagging\n",
        "To demonstrate how additional rules can improve tagging accuracy, consider the following rules:\n",
        "\n"
      ],
      "metadata": {
        "id": "VbKtrvQQronK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Certainly! Let's break down each regular expression pattern and its corresponding POS tag:\n",
        "\n",
        "1. **`(r'\\w+ly$', 'RB')`**:\n",
        "   - **Pattern**: `\\w+ly$`\n",
        "     - `\\w+` matches one or more word characters (letters, digits, or underscores).\n",
        "     - `ly` matches the characters \"ly\" literally.\n",
        "     - `$` asserts position at the end of the string.\n",
        "   - **Explanation**: This pattern matches adverbs ending with \"ly\", such as \"quickly\", \"beautifully\", \"normally\".\n",
        "   - **POS Tag**: `'RB'`\n",
        "     - **Explanation**: `'RB'` is the POS tag for adverbs in English. Adverbs modify verbs, adjectives, or other adverbs by providing information about manner, place, time, degree, etc.\n",
        "\n",
        "2. **`(r'[A-Z].*', 'NNP')`**:\n",
        "   - **Pattern**: `[A-Z].*`\n",
        "     - `[A-Z]` matches any uppercase letter.\n",
        "     - `.*` matches any character (except for line terminators) zero or more times.\n",
        "   - **Explanation**: This pattern matches proper nouns that start with an uppercase letter, such as \"John\", \"London\", \"Microsoft\".\n",
        "   - **POS Tag**: `'NNP'`\n",
        "     - **Explanation**: `'NNP'` is the POS tag for proper nouns in English. Proper nouns refer to specific names of people, places, organizations, etc.\n",
        "\n",
        "3. **`(r'\\w+ing$', 'VBG')`**:\n",
        "   - **Pattern**: `\\w+ing$`\n",
        "     - `\\w+` matches one or more word characters.\n",
        "     - `ing` matches the characters \"ing\" literally.\n",
        "     - `$` asserts position at the end of the string.\n",
        "   - **Explanation**: This pattern matches gerunds (verb forms ending in \"ing\"), such as \"walking\", \"talking\", \"swimming\".\n",
        "   - **POS Tag**: `'VBG'`\n",
        "     - **Explanation**: `'VBG'` is the POS tag for gerunds (present participles) in English. Gerunds are verb forms that function as nouns in sentences.\n",
        "\n",
        "4. **`(r'\\w+\\'s$', 'POS')`**:\n",
        "   - **Pattern**: `\\w+\\'s$`\n",
        "     - `\\w+` matches one or more word characters.\n",
        "     - `\\'s` matches the possessive form ending with \"'s\", such as \"dog's\", \"John's\".\n",
        "     - `$` asserts position at the end of the string.\n",
        "   - **Explanation**: This pattern matches possessive nouns ending in \"'s\".\n",
        "   - **POS Tag**: `'POS'`\n",
        "     - **Explanation**: `'POS'` is the POS tag for possessive endings in English. It denotes possession or association with the noun that precedes it.\n",
        "\n",
        "5. **`(r'and|but|or', 'CC')`**:\n",
        "   - **Pattern**: `and|but|or`\n",
        "     - Matches specific coordinating conjunctions: \"and\", \"but\", \"or\".\n",
        "   - **Explanation**: This pattern matches coordinating conjunctions that join words, phrases, or clauses of equal grammatical rank.\n",
        "   - **POS Tag**: `'CC'`\n",
        "     - **Explanation**: `'CC'` is the POS tag for coordinating conjunctions in English. Coordinating conjunctions connect words, phrases, or clauses together.\n",
        "\n",
        "### Summary:\n",
        "These regular expression patterns and their corresponding POS tags are used to enhance POS tagging in natural language processing tasks. They help identify specific linguistic patterns that may not be covered comprehensively by standard pretrained taggers, thereby improving accuracy in identifying adverbs, proper nouns, gerunds, possessive nouns, and coordinating conjunctions in text data."
      ],
      "metadata": {
        "id": "mpCZ3Uafr9dn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tag import RegexpTagger\n",
        "\n",
        "# Define additional rules for custom tagging using RegexpTagger\n",
        "additional_rules = [\n",
        "    (r'\\w+ly$', 'RB'),        # Adverbs ending in 'ly'\n",
        "    (r'[A-Z].*', 'NNP'),      # Proper nouns starting with uppercase\n",
        "    (r'\\w+ing$', 'VBG'),      # Gerunds ending in 'ing'\n",
        "    (r'\\w+\\'s$', 'POS'),     # Possessive nouns ending in \"'s\"\n",
        "    (r'and|but|or', 'CC')     # Coordinating conjunctions\n",
        "]\n",
        "\n",
        "# Create a RegexpTagger with additional rules\n",
        "regexp_tagger = RegexpTagger(additional_rules)\n",
        "\n",
        "# Apply the custom tagger to each sentence\n",
        "custom_tagged_sentences = []\n",
        "for sentence in sentences:\n",
        "    normalized_sentence = normalize_text(sentence)\n",
        "    tokens = word_tokenize(normalized_sentence)  # Tokenize the normalized sentence\n",
        "    custom_tags = regexp_tagger.tag(tokens)  # Use the custom RegexpTagger to get POS tags\n",
        "    custom_tagged_sentences.append(custom_tags)\n",
        "\n",
        "# Print custom POS tagged sentences\n",
        "print(\"Custom POS Tagging with Additional Rules:\")\n",
        "for i, tags in enumerate(custom_tagged_sentences, start=1):\n",
        "    print(f\"Sentence {i}:\")\n",
        "    print(tags)\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iju5O8HTrrxL",
        "outputId": "e6250a0f-3750-491a-8487-acc42a7efdb0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Custom POS Tagging with Additional Rules:\n",
            "Sentence 1:\n",
            "[('he', None), ('walked', None), ('quickly', 'RB'), ('to', None), ('the', None), ('store', None)]\n",
            "\n",
            "Sentence 2:\n",
            "[('she', None), ('sings', None), ('beautifully', 'RB'), ('every', None), ('morning', 'VBG')]\n",
            "\n",
            "Sentence 3:\n",
            "[('the', None), ('old', None), ('man', None), ('greeted', None), ('us', None), ('warmly', 'RB')]\n",
            "\n",
            "Sentence 4:\n",
            "[('their', None), ('ideas', None), ('seemed', None), ('quite', None), ('innovative', None)]\n",
            "\n",
            "Sentence 5:\n",
            "[('i', None), ('saw', None), ('a', None), ('man', None), ('with', None), ('a', None), ('telescope', None)]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using Different Corpora with NLTK\n",
        "NLTK provides access to various corpora for different languages and purposes. Here’s how you can access and work with some common corpora:\n",
        "\n",
        "Brown Corpus:\n",
        "\n",
        "Description: The Brown Corpus is a general corpus of English text, created in 1961 at Brown University.\n",
        "Access: You can access it using NLTK's corpus module:"
      ],
      "metadata": {
        "id": "k1j2jHlct4BJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import brown\n"
      ],
      "metadata": {
        "id": "34ZIDnCzt5Au"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gutenberg Corpus:\n",
        "\n",
        "Description: The Gutenberg Corpus includes a selection of literary works from Project Gutenberg.\n",
        "Access: Use NLTK to access it:"
      ],
      "metadata": {
        "id": "0y5ZMpdMt8TW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import gutenberg"
      ],
      "metadata": {
        "id": "SKss6137uA8c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inaugural Corpus:\n",
        "\n",
        "Description: The Inaugural Corpus includes U.S. presidential inaugural addresses.\n",
        "Access: Available through NLTK"
      ],
      "metadata": {
        "id": "2exnbq8IuDvD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import inaugural\n"
      ],
      "metadata": {
        "id": "QAvaFVCJuH6e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Web Text Corpus:\n",
        "\n",
        "Description: The Web Text Corpus contains text from a Firefox discussion forum.\n",
        "Access: Accessible through NLTK"
      ],
      "metadata": {
        "id": "yiMlNqDhuKRP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import webtext\n"
      ],
      "metadata": {
        "id": "5HWdFz9AuOjr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "r'[^\\w\\s]': This is the regular expression pattern being used. Let's break it down:\n",
        "\n",
        "[^\\w\\s]:\n",
        "[]: Denotes a character class (i.e., a set of characters to match).\n",
        "^: Inside a character class, the caret (^) negates the class, meaning \"match any character not in this set.\"\n",
        "\\w: Matches any word character (equivalent to [a-zA-Z0-9_]).\n",
        "\\s: Matches any whitespace character (spaces, tabs, newlines)."
      ],
      "metadata": {
        "id": "LugMv_tBpJgX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import brown\n",
        "nltk.download('brown')\n",
        "\n",
        "# Access the Brown Corpus\n",
        "words = brown.words()  # Get all words from the corpus\n",
        "text = ' '.join(words)  # Convert list of words into a single string (if needed)\n",
        "\n",
        "# Tokenization\n",
        "tokens = nltk.word_tokenize(text)\n",
        "\n",
        "# Cleaning (Normalization)\n",
        "def normalize_text(text):\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove punctuation using regex\n",
        "    import re\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    return text\n",
        "\n",
        "cleaned_tokens = [normalize_text(token) for token in tokens if token.isalpha()]\n",
        "\n",
        "# Example: Print first 10 cleaned tokens\n",
        "print(\"Cleaned Tokens:\")\n",
        "print(cleaned_tokens[:10])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sX3ulBImuTN0",
        "outputId": "e088982d-c3c8-497e-db36-2679f5a4fcbc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaned Tokens:\n",
            "['the', 'fulton', 'county', 'grand', 'jury', 'said', 'friday', 'an', 'investigation', 'of']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Treebank corpus is a widely used annotated corpus in computational linguistics and natural language processing (NLP). It is a collection of parsed and tagged sentences from the Wall Street Journal (WSJ) portion of the Penn Treebank, which is a large annotated corpus of English texts.\n",
        "\n",
        "Characteristics of the Treebank Corpus:\n",
        "Annotation: The Treebank corpus provides syntactic (parse trees) and morphological (POS tags) annotations for each sentence. Each word in the corpus is annotated with its part-of-speech tag, and sentences are parsed to show their syntactic structure.\n",
        "\n",
        "Size: The Treebank corpus is substantial, containing thousands of sentences from the WSJ. It is large enough to support the training and evaluation of various NLP models, particularly those related to POS tagging and syntactic parsing.\n",
        "\n",
        "Standardization: The annotations in the Treebank corpus adhere to standardized conventions and guidelines, making it a reliable resource for linguistic research and development of NLP algorithms.\n",
        "\n",
        "Applications: It is used extensively for training and evaluating POS taggers, syntactic parsers, and other NLP tools that require labeled data. Researchers and developers often use subsets of the Treebank corpus for specific experiments or tasks.\n",
        "\n",
        "Usage in Natural Language Processing:\n",
        "Training POS Taggers: Many POS taggers, such as UnigramTagger, BigramTagger, and ClassifierBasedPOSTagger in NLTK, are trained on subsets of the Treebank corpus to learn the statistical patterns of word-tag associations.\n",
        "\n",
        "Training Syntactic Parsers: The parsed sentences in the Treebank corpus are used to train and evaluate syntactic parsers that analyze the grammatical structure of sentences.\n",
        "\n",
        "Research and Development: Researchers and developers use the Treebank corpus as a benchmark dataset for testing the performance of new algorithms and techniques in NLP."
      ],
      "metadata": {
        "id": "fgflmGvCwZMT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import treebank\n",
        "from nltk.tag.sequential import ClassifierBasedPOSTagger\n",
        "from nltk.classify import NaiveBayesClassifier\n",
        "\n",
        "# Load the Treebank corpus\n",
        "nltk.download('treebank')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QICU5wPIqi8x",
        "outputId": "b0dbfc5c-d651-450b-b795-2443d1ef5979"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Package treebank is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = treebank.tagged_sents()\n",
        "print(sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PHZjZ15Wqlss",
        "outputId": "5b2bf347-26e1-479c-aa48-cb4f3ed90a53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[('Pierre', 'NNP'), ('Vinken', 'NNP'), (',', ','), ('61', 'CD'), ('years', 'NNS'), ('old', 'JJ'), (',', ','), ('will', 'MD'), ('join', 'VB'), ('the', 'DT'), ('board', 'NN'), ('as', 'IN'), ('a', 'DT'), ('nonexecutive', 'JJ'), ('director', 'NN'), ('Nov.', 'NNP'), ('29', 'CD'), ('.', '.')], [('Mr.', 'NNP'), ('Vinken', 'NNP'), ('is', 'VBZ'), ('chairman', 'NN'), ('of', 'IN'), ('Elsevier', 'NNP'), ('N.V.', 'NNP'), (',', ','), ('the', 'DT'), ('Dutch', 'NNP'), ('publishing', 'VBG'), ('group', 'NN'), ('.', '.')], ...]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exactly! Let's go deeper into what happens during training for the **Naive Bayes classifier** in POS tagging.\n",
        "\n",
        "### 1. **Feature Extraction and Counting**\n",
        "\n",
        "During training, the classifier gathers data about the **features** and their associated **POS tags** from the training set. This involves counting how often each feature appears with each POS tag.\n",
        "\n",
        "For example, let's say the model is training on the sentence `\"She is running quickly.\"` with the following POS tags:\n",
        "\n",
        "- `\"She\"` → `PRP` (Personal pronoun)\n",
        "- `\"is\"` → `VBZ` (Verb, 3rd person singular present)\n",
        "- `\"running\"` → `VBG` (Verb, gerund/present participle)\n",
        "- `\"quickly\"` → `RB` (Adverb)\n",
        "\n",
        "The features for each word might be extracted as follows (using your `pos_features` function):\n",
        "\n",
        "```python\n",
        "word: \"running\"\n",
        "features = {\n",
        "    'suffix(1)': 'g',           # Last character\n",
        "    'suffix(2)': 'ng',          # Last two characters\n",
        "    'suffix(3)': 'ing',         # Last three characters\n",
        "    'prev_word': 'is',          # Previous word\n",
        "    'next_word': 'quickly'      # Next word\n",
        "}\n",
        "```\n",
        "\n",
        "For each POS tag, the model counts:\n",
        "- How often **each feature** (e.g., suffixes like \"ing\" or previous/next word) appears **with a particular tag**.\n",
        "\n",
        "### 2. **Building Probability Distributions**\n",
        "Naive Bayes needs to estimate probabilities for each feature given a tag. To do this, it computes:\n",
        "\n",
        "#### - **P(feature | tag)**:\n",
        "For each POS tag, the classifier calculates the probability of seeing specific features. For example:\n",
        "- How often does the suffix \"ing\" appear with the tag `VBG` (present participle)?\n",
        "- How often is the word \"is\" followed by a word tagged as `VBG`?\n",
        "\n",
        "These probabilities are estimated using relative frequencies:\n",
        "\\[\n",
        "P(\\text{suffix}(3) = 'ing' | \\text{tag} = \\text{VBG}) = \\frac{\\text{Count of 'ing' with VBG}}{\\text{Total occurrences of VBG}}\n",
        "\\]\n",
        "\\[\n",
        "P(\\text{prev_word} = 'is' | \\text{tag} = \\text{VBG}) = \\frac{\\text{Count of 'is' preceding VBG}}{\\text{Total occurrences of VBG}}\n",
        "\\]\n",
        "\n",
        "For example, if:\n",
        "- The word **\"running\"** is tagged as `VBG` 200 times in the training data, and 150 of those occurrences have the suffix `\"ing\"`, then:\n",
        "  \\[\n",
        "  P(\\text{suffix}(3) = 'ing' | \\text{tag} = \\text{VBG}) = \\frac{150}{200} = 0.75\n",
        "  \\]\n",
        "- If the word **\"is\"** precedes a word tagged as `VBG` 50 times, and there are 200 instances of `VBG`, then:\n",
        "  \\[\n",
        "  P(\\text{prev_word} = 'is' | \\text{tag} = \\text{VBG}) = \\frac{50}{200} = 0.25\n",
        "  \\]\n",
        "\n",
        "#### - **P(tag)** (Prior Probability):\n",
        "The classifier also estimates the prior probability of each tag. For example, if `VBG` appears in 10% of all the tags in the training set:\n",
        "\\[\n",
        "P(\\text{tag} = \\text{VBG}) = 0.10\n",
        "\\]\n",
        "This prior helps adjust for the relative frequency of each tag (some tags are more common than others).\n",
        "\n",
        "### 3. **Combining the Probabilities (During Prediction)**\n",
        "After the classifier has been trained, it uses the learned probabilities to predict the tag for a word in a new sentence.\n",
        "\n",
        "Given the features for the word **\"running\"**, the model calculates the probability of each possible tag by multiplying the probabilities of the features given that tag (based on the counts from training) and the prior probability of the tag:\n",
        "\n",
        "For `VBG`:\n",
        "\\[\n",
        "P(\\text{VBG} | \\text{features}) \\propto P(\\text{features} | \\text{VBG}) \\times P(\\text{VBG})\n",
        "\\]\n",
        "This includes:\n",
        "\\[\n",
        "P(\\text{suffix}(3) = 'ing' | \\text{VBG}) \\times P(\\text{prev_word} = 'is' | \\text{VBG}) \\times P(\\text{VBG})\n",
        "\\]\n",
        "\n",
        "For each other possible tag (e.g., `NN`, `VBZ`), the classifier calculates a similar probability:\n",
        "\\[\n",
        "P(\\text{NN} | \\text{features}), P(\\text{VBZ} | \\text{features}), \\dots\n",
        "\\]\n",
        "Then, it picks the tag with the highest probability.\n",
        "\n",
        "### 4. **Smoothing**:\n",
        "Sometimes, a feature might not appear with a specific tag in the training set. For example, there may be no examples where the suffix \"ing\" occurs with `NN` (noun). In such cases, the probability `P(suffix(3) = 'ing' | NN)` could be zero, which would lead to incorrect predictions.\n",
        "\n",
        "To handle this, Naive Bayes typically uses **smoothing** (such as Laplace smoothing), which assigns a small non-zero probability to unseen events to avoid zero probabilities in the calculation.\n",
        "\n",
        "### Example of Feature-Tag Probabilities (Hypothetical):\n",
        "\n",
        "| Feature                  | Tag  | Count   | Probability \\(P(\\text{feature}|\\text{tag})\\) |\n",
        "|--------------------------|------|---------|----------------------------------------------|\n",
        "| `suffix(3) = 'ing'`       | VBG  | 150     | 0.75                                         |\n",
        "| `suffix(3) = 'ing'`       | NN   | 10      | 0.05                                         |\n",
        "| `prev_word = 'is'`        | VBG  | 50      | 0.25                                         |\n",
        "| `next_word = 'quickly'`   | VBG  | 20      | 0.10                                         |\n",
        "| `suffix(1) = 's'`         | NNS  | 120     | 0.60                                         |\n",
        "\n",
        "Using these probabilities, the classifier combines them to determine the most likely tag.\n",
        "\n",
        "### Conclusion:\n",
        "\n",
        "- During training, the Naive Bayes classifier calculates **feature frequencies** for each tag from the training data.\n",
        "- It builds **probability distributions** from these counts, estimating the likelihood of each feature occurring with each tag.\n",
        "- The classifier then uses these probabilities to make predictions on new data by combining the probabilities for each feature and selecting the tag that maximizes the likelihood."
      ],
      "metadata": {
        "id": "eSOdes4oxbil"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import treebank\n",
        "from nltk.tag.sequential import ClassifierBasedPOSTagger\n",
        "from nltk.classify import NaiveBayesClassifier\n",
        "\n",
        "# Load the Treebank corpus\n",
        "nltk.download('treebank')\n",
        "sentences = treebank.tagged_sents()\n",
        "\n",
        "# Split into training and test sets (80-20 split)\n",
        "train_data = sentences[:int(len(sentences) * 0.8)]\n",
        "test_data = sentences[int(len(sentences) * 0.8):]\n",
        "\n",
        "\n",
        "# Define features for the custom tagger\n",
        "def pos_features(sentence, index):\n",
        "    features = {\n",
        "        'suffix(1)': sentence[index][-1:],\n",
        "        'suffix(2)': sentence[index][-2:],\n",
        "        'suffix(3)': sentence[index][-3:],\n",
        "        'prev_word': '' if index == 0 else sentence[index - 1],\n",
        "        'next_word': '' if index == len(sentence) - 1 else sentence[index + 1]\n",
        "    }\n",
        "    return features\n",
        "\n",
        "# Prepare training data with features\n",
        "train_data_features = []\n",
        "for sent in train_data:\n",
        "    untagged_sent = nltk.tag.untag(sent)\n",
        "    for i, (word, tag) in enumerate(sent):\n",
        "        featureset = (pos_features(untagged_sent, i), tag)\n",
        "        train_data_features.append(featureset)\n",
        "\n",
        "# Train a custom POS tagger\n",
        "classifier = NaiveBayesClassifier.train(train_data_features)\n",
        "custom_tagger = ClassifierBasedPOSTagger(classifier=classifier)\n",
        "\n",
        "# Evaluate the custom tagger\n",
        "accuracy = custom_tagger.evaluate(test_data)\n",
        "print(f\"Accuracy: {accuracy:.2%}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pfbg4kQVwOKV",
        "outputId": "e0f0d496-f051-4995-b292-5ef1aada2f77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Package treebank is already up-to-date!\n",
            "<ipython-input-24-71248bfcb22b>:39: DeprecationWarning: \n",
            "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
            "  instead.\n",
            "  accuracy = custom_tagger.evaluate(test_data)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 14.48%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import treebank\n",
        "from nltk.tag.sequential import ClassifierBasedPOSTagger\n",
        "from nltk.classify import NaiveBayesClassifier\n",
        "\n",
        "# Load the Treebank corpus\n",
        "nltk.download('treebank')\n",
        "sentences = treebank.tagged_sents()\n",
        "print(sentences[:2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zus1_Upkfx-c",
        "outputId": "833f6488-a548-414b-da26-7a42f584c067"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[('Pierre', 'NNP'), ('Vinken', 'NNP'), (',', ','), ('61', 'CD'), ('years', 'NNS'), ('old', 'JJ'), (',', ','), ('will', 'MD'), ('join', 'VB'), ('the', 'DT'), ('board', 'NN'), ('as', 'IN'), ('a', 'DT'), ('nonexecutive', 'JJ'), ('director', 'NN'), ('Nov.', 'NNP'), ('29', 'CD'), ('.', '.')], [('Mr.', 'NNP'), ('Vinken', 'NNP'), ('is', 'VBZ'), ('chairman', 'NN'), ('of', 'IN'), ('Elsevier', 'NNP'), ('N.V.', 'NNP'), (',', ','), ('the', 'DT'), ('Dutch', 'NNP'), ('publishing', 'VBG'), ('group', 'NN'), ('.', '.')]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Package treebank is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print tagged sentences with custom rules applied\n",
        "for sent in test_data[:5]:  # Print first 5 sentences\n",
        "    print(sent)\n"
      ],
      "metadata": {
        "id": "MiJQiTQxaFvm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab388ca5-ad58-4a20-9957-b057a350b83f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('The', 'DT'), ('latest', 'JJS'), ('10-year', 'JJ'), ('notes', 'NNS'), ('were', 'VBD'), ('quoted', 'VBN'), ('at', 'IN'), ('100', 'CD'), ('22\\\\/32', 'CD'), ('*-1', '-NONE-'), ('to', 'TO'), ('yield', 'VB'), ('7.88', 'CD'), ('%', 'NN'), ('compared', 'VBN'), ('with', 'IN'), ('100', 'CD'), ('16\\\\/32', 'CD'), ('*', '-NONE-'), ('to', 'TO'), ('yield', 'VB'), ('7.90', 'CD'), ('%', 'NN'), ('.', '.')]\n",
            "[('The', 'DT'), ('discount', 'NN'), ('rate', 'NN'), ('on', 'IN'), ('three-month', 'JJ'), ('Treasury', 'NNP'), ('bills', 'NNS'), ('was', 'VBD'), ('essentially', 'RB'), ('unchanged', 'JJ'), ('at', 'IN'), ('7.79', 'CD'), ('%', 'NN'), (',', ','), ('while', 'IN'), ('the', 'DT'), ('rate', 'NN'), ('on', 'IN'), ('six-month', 'JJ'), ('bills', 'NNS'), ('was', 'VBD'), ('slightly', 'RB'), ('lower', 'JJR'), ('at', 'IN'), ('7.52', 'CD'), ('%', 'NN'), ('compared', 'VBN'), ('with', 'IN'), ('7.60', 'CD'), ('%', 'NN'), ('Tuesday', 'NNP'), ('.', '.')]\n",
            "[('Corporate', 'NNP'), ('Issues', 'NNPS')]\n",
            "[('IBM', 'NNP'), (\"'s\", 'POS'), ('$', '$'), ('750', 'CD'), ('million', 'CD'), ('*U*', '-NONE-'), ('debenture', 'NN'), ('offering', 'NN'), ('dominated', 'VBD'), ('activity', 'NN'), ('in', 'IN'), ('the', 'DT'), ('corporate', 'JJ'), ('debt', 'NN'), ('market', 'NN'), ('.', '.')]\n",
            "[('Meanwhile', 'RB'), (',', ','), ('most', 'RBS'), ('investment-grade', 'JJ'), ('bonds', 'NNS'), ('ended', 'VBD'), ('unchanged', 'JJ'), ('to', 'TO'), ('as', 'RB'), ('much', 'JJ'), ('as', 'IN'), ('1\\\\/8', 'CD'), ('point', 'NN'), ('higher', 'JJR'), ('.', '.')]\n"
          ]
        }
      ]
    }
  ]
}